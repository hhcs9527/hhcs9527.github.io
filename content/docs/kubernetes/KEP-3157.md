---
title: "KEP-3157 Watch List"
type: docs
---
## Why KEP-3157
In large clusters, the LIST+WATCH patterns would cause 2 problems:
1. **Memory Explosion**: Full LIST requests load all resources into memory at once. With thousands of resources, this causes gigabytes of memory consumption per request, risking API server crashes.

2. **Stale Reads**: Traditional LIST uses RV="0", which reads from watchCache without consistency guarantees, potentially serving stale data.

KEP 3157 addresses these issues by introducing a streaming-based approach that:
- Ensures controllers receive consitent, up-to-date data by RV="", avoid reading **stale data**
- Replace memory-intensive LIST+WATCH pattern with single streaming WATCH request


## HA with Stacked etcd topology
KEP-3157 focuses on:

- Memory Efficiency in watchCache
- **Consistency Guarantee** between watchCache across the cluster
- Prevent controller watcher read the stale data.

```mermaid
graph BT
    subgraph ETCD_CLUSTER["etcd cluster"]
      ETCD1[(etcd<br/>actual data store)]
      ETCD2[(etcd<br/>actual data store)]
      ETCD3[(etcd<br/>actual data store)]
    end

    subgraph API_SERVER_1["API Server 1"]
      WC1[watchCache 1]
    end

    subgraph API_SERVER_2["API Server 2"]
      WC2[watchCache 3]
    end

    subgraph API_SERVER_3["API Server 3"]
      WC3[watchCache 3]
    end

    LB[Load Balancer]

    API_SERVER_1 <--quorum read / heartbeat--> ETCD1
    API_SERVER_2 <--quorum read / heartbeat--> ETCD2
    API_SERVER_3 <--quorum read / heartbeat--> ETCD3

    LB --> API_SERVER_1
    LB --> API_SERVER_2
    LB --> API_SERVER_3

    CW1[Controller watcher1] <--WATCH RV=&quot;&quot;--> LB
    CW2[Controller watcher2] <--WATCH RV=&quot;&quot;--> LB
    CW3[Controller watcher3] <--WATCH RV=&quot;&quot;--> LB
    CW4[Controller watcher4] <--WATCH RV=&quot;&quot;--> LB

    CTRL1[Controller 1] --> CW1
    CTRL2[Controller 2] --> CW2
    CTRL3[Controller 3] --> CW3
    CTRL4[Controller 4] --> CW4

    style WC1 stroke:#2196f3,stroke-width:3px
    style WC2 stroke:#2196f3,stroke-width:3px
    style WC3 stroke:#2196f3,stroke-width:3px

    style CW1 stroke:#4caf50,stroke-width:3px
    style CW2 stroke:#4caf50,stroke-width:3px
    style CW3 stroke:#4caf50,stroke-width:3px
    style CW4 stroke:#4caf50,stroke-width:3px
```

## ResourceVersion Semantics


| Semantics | `RV=""` | `RV="0"` |
|------|---------|----------|
| Source | etcd quorum read | Watch Cache |
| Latency | High | Low |
| Data Fresh | latest | Can be stale |
| Consistency | Strong Consistency | Eventual concistwency |
| Use case | Initializartion the wattchCache | Monitoring |

### `RV=""` - Strong
- **FLow**: etcd **Quorum Read**
- **Why CP**
  - 保證獲得最新提交的資料


### `RV="0"` - 最佳努力可用性
- **Flow**: Read **Watch Cache**, if empty fall back to etcd **Quorum Read**
- **Why AP**
  - 保證獲得最新提交的資料

## Consistency Guarantee Mechanism
## Latency Analysis in Watch-List

## Problem
For every watcher to read the data from informer, it require to List the data first, then watch the event changes, which brought
	- If there are many watcher, it will operate the same things.
	- Each List will do the following
        1. LIST RV="0" request
           - Require the entire etcd read, which will cause the high mem usage for loading the data
           - The moment where data read it will has the issue, may not consist
        2. WATCH the resource
        3. When the data is either expired or compact, will start from #1


## Solution
Watch list

## Lession learn

### Why does the stream can alleviate the memeory
- stream background is ?
- code where they use the stream
- code experiment, vanilla v.s. my own implementation
### How to get the alert for the apiserver?
### step2.
- upon receiving the request from an informer, contact etcd to get the latest RV. It will be used to make sure the watch cache has seen objects up to the received RV. This step is necessary and ensures we will serve consistent data, even from the cache.
- why can we ensure it upto date?
- cold start of etcd?

### How does sample apiserver works on watch list implementation?

### bookmark event
### list RV = 0



### Env
### Code to test it
### Code to test it
<a id="bookmark-ref"></a>
- … [bookmark](#bookmark-note)

<a id="rv-empty-ref"></a>
- … [RV=""](#rv-empty-note)

<a id="rv-0-ref"></a>
- … [RV=0](#rv-0-note)


### trade off using the RV=0 with CAP
# ResourceVersion 參數對比

## 核心差異



## KEP 3157 的改進

新的 Watch-List 方法結合兩者優勢：

```
1. 使用 RV="" 建立一致性基準點
2. 等待 Watch Cache 同步到該基準點
3. 從 Watch Cache 進行高效串流傳輸
```

這種方式既保證了一致性，又避免了記憶體爆炸問題。

## How does it affect to the watcher?
overall flow it
controller -> informer -> [reflector](#relector-note) -> API server -> etcd

## Experiment

## Reference
- [KEP-3157](https://github.com/kubernetes/enhancements/blob/master/keps/sig-api-machinery/3157-watch-list/README.md)

---

### Note
- <a id="bookmark-note"></a> **Bookmark Event** — a Kubernetes watch event that notifies the client of the query object's resource version. [↩](#bookmark-ref)

- <a id="rv-empty-note"></a> RV="" — etcd issues a quorum read, guaranteeing the latest committed data.

- <a id="rv-0-note"></a> RV=0 — etcd serves the request without quorum; the response may be stale.

- <a id="rv-0-note"></a> reflector — responsible for sync the data and build the list watch to help the controller get the to upto dcate date.